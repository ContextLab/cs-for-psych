{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubrics = ['public_rubric.xls', 'private_rubric.xls']\n",
    "target = 'assignment_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'ipython nbconvert --to python {target}.ipynb');\n",
    "exec(f'import {target} as submission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_script(target):\n",
    "    script_already_exists = os.path.exists(target + '.py')\n",
    "    if script_already_exists:\n",
    "        os.renames(target + '.py', target + '.py.BACKUP')\n",
    "\n",
    "    os.system(f'ipython nbconvert --to python {target}.ipynb');\n",
    "    \n",
    "    exec(f'import {target} as submission')\n",
    "    \n",
    "    return submission, script_already_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub, already_exists = convert_to_script('assignment_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_str(s):\n",
    "    try:\n",
    "        if s is np.nan:\n",
    "            return ''\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if s is None:\n",
    "            return ''\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        if len(s) == 0:\n",
    "            return ''\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade(x, rubric):    \n",
    "    if type(rubric) == list:\n",
    "        passed = []\n",
    "        failed = []\n",
    "        for r in rubric:\n",
    "            next_passed, next_failed = grade(x, r)\n",
    "            passed.extend(next_passed)\n",
    "            failed.extend(next_failed)\n",
    "        return passed, failed\n",
    "    \n",
    "    r = pd.read_excel(rubric, header=0)\n",
    "    \n",
    "    passed = []\n",
    "    failed = []\n",
    "    \n",
    "    for i in range(r.shape[0]):\n",
    "        next_test = {}\n",
    "        if r.loc[i]['function'] == 'identity':\n",
    "            next_test['cmd'] = f'x.{r.loc[i]['input']} == {from_str(r.loc[i]['solution'])}'\n",
    "            next_test['target'] = True)\n",
    "        else:\n",
    "            next_test['cmd'] = f\"x.{r.loc[i]['function']}({str(from_str(r.loc[i]['input']))})\"\n",
    "            next_test['target'] = from_str(r.loc[i]['solution'])\n",
    "        next_test['points'] = float(r.loc[i]['points'])\n",
    "        next_test['rubric'] = rubric\n",
    "        \n",
    "        #FIXME: this part is broken...\n",
    "        try:\n",
    "            next_test['response'] = eval(next_test['cmd'])            \n",
    "        except:\n",
    "            next_test['response'] = None        \n",
    "        try:            \n",
    "            if next_test['response'] == next_test['target']:\n",
    "                passed.append(next_test)\n",
    "            else:\n",
    "                failed.append(next_test)\n",
    "        except:\n",
    "            try:\n",
    "                if np.isnan(next_test['target']):\n",
    "                    passed.append(next_test)\n",
    "                else:\n",
    "                    failed.append(next_test)\n",
    "            except:\n",
    "                failed.append(next_test)\n",
    "    \n",
    "    return passed, failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(passed, failed):\n",
    "    possible = 0\n",
    "    earned = 0\n",
    "    \n",
    "    def summarize(description, cases):\n",
    "        if len(cases) == 0:\n",
    "            return f'No cases {description.lower()}.'\n",
    "        \n",
    "        summary = [f'The following test cases {description}:']\n",
    "        for i, c in enumerate(cases):\n",
    "            summary.append(f\"{i+1}. Command: {c['cmd'][2:]}\")\n",
    "            summary.append(f\"\\t Observed output: {c['response']}\")\n",
    "            summary.append(f\"\\t Expected output: {c['target']}\")\n",
    "            summary.append(f\"\\t Points: {c['points']}\")\n",
    "            summary.append('')\n",
    "    \n",
    "        return '\\n'.join(summary[:-1])\n",
    "    \n",
    "    for p in passed:\n",
    "        earned += p['points']\n",
    "        possible += p['points']\n",
    "    \n",
    "    for f in failed:\n",
    "        possible += f['points']\n",
    "    \n",
    "    print(f'Total points earned: {earned}/{possible} = {100 * np.round(float(earned)/(float(possible)), decimals=4)}%')\n",
    "    print('\\nDetails:\\n')\n",
    "    print(summarize('PASSED', passed))\n",
    "    print('\\n')\n",
    "    print(summarize('FAILED', failed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total points earned: 17.5/26.75 = 65.42%\n",
      "\n",
      "Details:\n",
      "\n",
      "The following test cases PASSED:\n",
      "1. Command: say_hello()\n",
      "\t Observed output: hello\n",
      "\t Expected output: hello\n",
      "\t Points: 0.5\n",
      "\n",
      "2. Command: sum_1_n(3)\n",
      "\t Observed output: 6\n",
      "\t Expected output: 6\n",
      "\t Points: 1.0\n",
      "\n",
      "3. Command: sum_1_n(100)\n",
      "\t Observed output: 5050\n",
      "\t Expected output: 5050\n",
      "\t Points: 1.0\n",
      "\n",
      "4. Command: sum_1_n(-10)\n",
      "\t Observed output: 0\n",
      "\t Expected output: 0\n",
      "\t Points: 2.0\n",
      "\n",
      "5. Command: sum_1_n(0.5)\n",
      "\t Observed output: 0\n",
      "\t Expected output: 0\n",
      "\t Points: 1.0\n",
      "\n",
      "6. Command: sum_1_n([1, 2, 3])\n",
      "\t Observed output: [1, 3, 6]\n",
      "\t Expected output: [1, 3, 6]\n",
      "\t Points: 2.0\n",
      "\n",
      "7. Command: sum_1_n_no_checks(3)\n",
      "\t Observed output: 6\n",
      "\t Expected output: 6\n",
      "\t Points: 1.0\n",
      "\n",
      "8. Command: sum_1_n_no_checks(100)\n",
      "\t Observed output: 5050\n",
      "\t Expected output: 5050\n",
      "\t Points: 1.0\n",
      "\n",
      "9. Command: sum_1_n_no_checks(-10)\n",
      "\t Observed output: 0\n",
      "\t Expected output: 0\n",
      "\t Points: 2.0\n",
      "\n",
      "10. Command: sum_1_n([-100, 30, 7, 50])\n",
      "\t Observed output: [0, 465, 28, 1275]\n",
      "\t Expected output: [0, 465, 28, 1275]\n",
      "\t Points: 2.0\n",
      "\n",
      "11. Command: sum_1_n(1000000)\n",
      "\t Observed output: 500000500000\n",
      "\t Expected output: 500000500000\n",
      "\t Points: 2.0\n",
      "\n",
      "12. Command: sum_1_n_no_checks(1000000)\n",
      "\t Observed output: 500000500000\n",
      "\t Expected output: 500000500000\n",
      "\t Points: 2.0\n",
      "\n",
      "\n",
      "The following test cases FAILED:\n",
      "1. Command: sum_1_n(hello)\n",
      "\t Observed output: None\n",
      "\t Expected output: 0\n",
      "\t Points: 2.0\n",
      "\n",
      "2. Command: sum_1_n_no_checks(0.5)\n",
      "\t Observed output: None\n",
      "\t Expected output: 0\n",
      "\t Points: 1.0\n",
      "\n",
      "3. Command: sum_1_n_no_checks(hello)\n",
      "\t Observed output: None\n",
      "\t Expected output: 0\n",
      "\t Points: 2.0\n",
      "\n",
      "4. Command: sum_1_n_no_checks([1, 2, 3])\n",
      "\t Observed output: None\n",
      "\t Expected output: [1, 3, 6]\n",
      "\t Points: 2.0\n",
      "\n",
      "5. Command: say_hello(goodbye)\n",
      "\t Observed output: None\n",
      "\t Expected output: \n",
      "\t Points: 0.25\n",
      "\n",
      "6. Command: sum_1_n_no_checks([-100, 30, 7, 50])\n",
      "\t Observed output: None\n",
      "\t Expected output: [0, 465, 28, 1275]\n",
      "\t Points: 2.0\n"
     ]
    }
   ],
   "source": [
    "passed, failed = grade(submission, rubrics)\n",
    "report(passed, failed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
