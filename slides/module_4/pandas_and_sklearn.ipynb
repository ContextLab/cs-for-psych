{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pandas_and_sklearn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP3og6oxF1T4sg5kREk4AN7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ContextLab/cs-for-psych/blob/master/slides/module_4/pandas_and_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q3m71HzHN5z"
      },
      "source": [
        "# Exploring `pandas` and `scikit-learn`\n",
        "\n",
        "In this notebook we'll explore some more advanced features of two popular Python libraries:\n",
        "- [pandas](https://pandas.pydata.org/): **P**ytho**N** **DA**ta **S**cience Library is essentially a wrapper for [numpy](https://numpy.org/) that provides really useful tools for bookkeeping and manipulating data.  If you're familiar with [`data.frame`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/data.frame) objects or the [`plyr`](https://www.rdocumentation.org/packages/plyr/versions/1.8.6) or [`dplyr`](https://dplyr.tidyverse.org/) libraries in R, you'll see lots of similarities to Pandas `DataFrame` objects and how to work with them.  As a historical note, Pandas was inspired by R dataframes and R's \"[tidyverse](https://www.tidyverse.org/)\" libraries for data science.\n",
        "- [scikit-learn](https://scikit-learn.org/stable/): a machine learning package for Python that implements many of the most widely used algorithms for classification, regression, clustering, dimensionality reduction, model selection, and preprocessing.  Scikit-learn is built on top of [numpy](https://numpy.org/), [pandas](https://pandas.pydata.org/), and [matplotlib](https://matplotlib.org/stable/index.html).\n",
        "\n",
        "Before digging into either of these libraries, here are some useful orienting guidelines that might help you to think about what each library does:\n",
        "- Numpy's main feature is that it introduces the `array` datatype.  Arrays are like matrices or tensors-- *n*-dimensional tables of numbers.From an implementation standpoint, arrays are just like nested lists, where the outer-most level corresponds to the first dimension, the second outer-most level corresponds to the second dimension, and so on (where the inner-most level corresponds to the last dimension).  Because arrays are built on lists, you can often pass nested lists directly into numpy functions (as though they were arrays) and they'll be treated just like arrays.  Once you've organized your data into one or more arrays (or array-like objects), numpy allows you to apply a wide variety of linear algebra operations to the data.  Numpy is also written in efficient C code, which means that you can work effectively with very large datasets.\n",
        "- Pandas's main feature is the introduction of the `DataFrame` datatype.  From an implementation standpoint, DataFrames are like Python dictionaries where one key, called \"`values`\" points to the data (stored as a 2D numpy array); a second key, called \"`index`\" points to another numpy array that contains labels for the rows; and a third key, called \"`columns`\" that points to a numpy array containing labels for the columns.  Once you've created a DataFrame, you can mostly treat it just like a 2D numpy array-- many numpy functions \"just work\" on DataFrames (e.g., it'll usually work fine if you just pass DataFrames directly to numpy methods that are expecting arrays).\n",
        "- Scikit-learn really provides two general tools:\n",
        "  - First, the library includes implementations of a wide variety of algorithms for doing the most widely used machine learning tasks.  This typically requires your data to be organized into numpy arrays and/or pandas DataFrames and or lists.  \n",
        "  - Second, the library includes a general framework for *organizing* code for implementing machine learning algorithms.  This is arguably the most powerful and furthest-reaching contribution of scikit-learn.  For example, nearly every model in scikit-learn is implemented as a Python class with a `fit` method (which takes in a training dataset and trains or applies the given model) and a `transform` method (which takes in potentially new data and projects it through the given model).  The common structure across models means that it's relatively straightforward to implement new models or features that will then play nicely with the other functionality in the library.\n",
        "    - Additional related libraries extend the functionality of scikit-learn even further.  For example, [scikit-image](https://scikit-image.org/) adds image processing algorithms; [scikit-network](https://scikit-network.readthedocs.io/en/latest/) adds graph theory algorithms; [scikit-optimize](https://scikit-optimize.github.io/stable/) adds some additional optimization algorithms; and so on.\n",
        "    - There is some redundancy between scikit-learn and other popular libraries.  For example, scikit-learn includes some deep learning models and tools.  However, most of these implementations are less efficient and less flexible than libraries like [tensorflow](https://www.tensorflow.org/) or [pytorch](https://pytorch.org/) that are focused specifically on deep learning, rather than on \"machine learning\" in general.\n",
        "    - A reasonable rule of thumb might be to implement basic ideas using scikit-learn as a way to get things \"up and running\" on a test dataset or application.  But then if you want to scale things up to a much larger dataset you may want to port things over to another library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szewskbxhMpo"
      },
      "source": [
        "# Library imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cscV1kAwGIBv"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn as skl\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS8LsPpdSEPp"
      },
      "source": [
        "# Datasets\n",
        "\n",
        "We'll play around with a \"toy\" dataset included with Seaborn:\n",
        "  - A list of 891 Titanic passengers and various compiled pieces of information about then\n",
        "\n",
        "We'll also look at two datasets from [fivethirtyeight](https://fivethirtyeight.com/):\n",
        "  - Guests that appeared on Jon Stewart's 'The Daily Show' (inspired by [this article](https://fivethirtyeight.com/features/every-guest-jon-stewart-ever-had-on-the-daily-show/)\n",
        "  - Superbowl commercials (inspired by [this article](https://projects.fivethirtyeight.com/super-bowl-ads/))\n",
        "\n",
        "We can use the pandas [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) function to load in data stored in a CSV file.  Analogous pandas functions can read in data stored in a wide variety of formats including Excel, JSON, HDF, SAS, SPSS, SQL, BigQuery, STATA, [and more](https://pandas.pydata.org/pandas-docs/stable/reference/io.html). Most of these functions support reading both from locally stored files *or* directly from a remote URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE0UL1DljMGK"
      },
      "source": [
        "titanic = sns.load_dataset('titanic')\n",
        "daily_show_guests = pd.read_csv('https://raw.githubusercontent.com/fivethirtyeight/data/master/daily-show-guests/daily_show_guests.csv', header=0)\n",
        "superbowl_ads = pd.read_csv('https://raw.githubusercontent.com/fivethirtyeight/superbowl-ads/main/superbowl-ads.csv', header=0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HREWHt3Ajgh3"
      },
      "source": [
        "It's always good to check that the dataset was loaded in correctly; I like to use the `head` function (which prints out the first 5 rows of the DataFrame by default; you can customize how many lines are printed by passing in any non-negative integer).  The `tail` function behaves similarly, but it prints out the *last* rows of the table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6Qe8cKKc-M0g",
        "outputId": "259b0699-ec41-4fb7-a892-39a9a8e39188"
      },
      "source": [
        "titanic.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>pclass</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>embarked</th>\n",
              "      <th>class</th>\n",
              "      <th>who</th>\n",
              "      <th>adult_male</th>\n",
              "      <th>deck</th>\n",
              "      <th>embark_town</th>\n",
              "      <th>alive</th>\n",
              "      <th>alone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>S</td>\n",
              "      <td>Third</td>\n",
              "      <td>man</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>no</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C</td>\n",
              "      <td>First</td>\n",
              "      <td>woman</td>\n",
              "      <td>False</td>\n",
              "      <td>C</td>\n",
              "      <td>Cherbourg</td>\n",
              "      <td>yes</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>S</td>\n",
              "      <td>Third</td>\n",
              "      <td>woman</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>yes</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>S</td>\n",
              "      <td>First</td>\n",
              "      <td>woman</td>\n",
              "      <td>False</td>\n",
              "      <td>C</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>yes</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>S</td>\n",
              "      <td>Third</td>\n",
              "      <td>man</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>no</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   survived  pclass     sex   age  ...  deck  embark_town  alive  alone\n",
              "0         0       3    male  22.0  ...   NaN  Southampton     no  False\n",
              "1         1       1  female  38.0  ...     C    Cherbourg    yes  False\n",
              "2         1       3  female  26.0  ...   NaN  Southampton    yes   True\n",
              "3         1       1  female  35.0  ...     C  Southampton    yes  False\n",
              "4         0       3    male  35.0  ...   NaN  Southampton     no   True\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "sFYlDpZzjMAz",
        "outputId": "cedc00c0-d0b7-43ef-ce10-2b1b4048d99b"
      },
      "source": [
        "daily_show_guests.head(20)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>YEAR</th>\n",
              "      <th>GoogleKnowlege_Occupation</th>\n",
              "      <th>Show</th>\n",
              "      <th>Group</th>\n",
              "      <th>Raw_Guest_List</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1999</td>\n",
              "      <td>actor</td>\n",
              "      <td>1/11/99</td>\n",
              "      <td>Acting</td>\n",
              "      <td>Michael J. Fox</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1999</td>\n",
              "      <td>Comedian</td>\n",
              "      <td>1/12/99</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>Sandra Bernhard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1999</td>\n",
              "      <td>television actress</td>\n",
              "      <td>1/13/99</td>\n",
              "      <td>Acting</td>\n",
              "      <td>Tracey Ullman</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1999</td>\n",
              "      <td>film actress</td>\n",
              "      <td>1/14/99</td>\n",
              "      <td>Acting</td>\n",
              "      <td>Gillian Anderson</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1999</td>\n",
              "      <td>actor</td>\n",
              "      <td>1/18/99</td>\n",
              "      <td>Acting</td>\n",
              "      <td>David Alan Grier</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1999</td>\n",
              "      <td>actor</td>\n",
              "      <td>1/19/99</td>\n",
              "      <td>Acting</td>\n",
              "      <td>William Baldwin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1999</td>\n",
              "      <td>Singer-lyricist</td>\n",
              "      <td>1/20/99</td>\n",
              "      <td>Musician</td>\n",
              "      <td>Michael Stipe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1999</td>\n",
              "      <td>model</td>\n",
              "      <td>1/21/99</td>\n",
              "      <td>Media</td>\n",
              "      <td>Carmen Electra</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1999</td>\n",
              "      <td>actor</td>\n",
              "      <td>1/25/99</td>\n",
              "      <td>Acting</td>\n",
              "      <td>Matthew Lillard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1999</td>\n",
              "      <td>stand-up comedian</td>\n",
              "      <td>1/26/99</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>David Cross</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1999</td>\n",
              "      <td>actress</td>\n",
              "      <td>1/27/99</td>\n",
              "      <td>Acting</td>\n",
              "      <td>Yasmine Bleeth</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1999</td>\n",
              "      <td>actor</td>\n",
              "      <td>1/28/99</td>\n",
              "      <td>Acting</td>\n",
              "      <td>D. L. Hughley</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1999</td>\n",
              "      <td>television actress</td>\n",
              "      <td>10/18/99</td>\n",
              "      <td>Acting</td>\n",
              "      <td>Rebecca Gayheart</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1999</td>\n",
              "      <td>Comedian</td>\n",
              "      <td>10/19/99</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>Steven Wright</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1999</td>\n",
              "      <td>actress</td>\n",
              "      <td>10/20/99</td>\n",
              "      <td>Acting</td>\n",
              "      <td>Amy Brenneman</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1999</td>\n",
              "      <td>actress</td>\n",
              "      <td>10/21/99</td>\n",
              "      <td>Acting</td>\n",
              "      <td>Melissa Gilbert</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1999</td>\n",
              "      <td>actress</td>\n",
              "      <td>10/25/99</td>\n",
              "      <td>Acting</td>\n",
              "      <td>Cathy Moriarty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1999</td>\n",
              "      <td>comedian</td>\n",
              "      <td>10/26/99</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>Louie Anderson</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1999</td>\n",
              "      <td>actress</td>\n",
              "      <td>10/27/99</td>\n",
              "      <td>Acting</td>\n",
              "      <td>Sarah Michelle Gellar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1999</td>\n",
              "      <td>Singer-songwriter</td>\n",
              "      <td>10/28/99</td>\n",
              "      <td>Musician</td>\n",
              "      <td>Melanie C</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    YEAR GoogleKnowlege_Occupation      Show     Group         Raw_Guest_List\n",
              "0   1999                     actor   1/11/99    Acting         Michael J. Fox\n",
              "1   1999                  Comedian   1/12/99    Comedy        Sandra Bernhard\n",
              "2   1999        television actress   1/13/99    Acting          Tracey Ullman\n",
              "3   1999              film actress   1/14/99    Acting       Gillian Anderson\n",
              "4   1999                     actor   1/18/99    Acting       David Alan Grier\n",
              "5   1999                     actor   1/19/99    Acting        William Baldwin\n",
              "6   1999           Singer-lyricist   1/20/99  Musician          Michael Stipe\n",
              "7   1999                     model   1/21/99     Media         Carmen Electra\n",
              "8   1999                     actor   1/25/99    Acting        Matthew Lillard\n",
              "9   1999         stand-up comedian   1/26/99    Comedy            David Cross\n",
              "10  1999                   actress   1/27/99    Acting         Yasmine Bleeth\n",
              "11  1999                     actor   1/28/99    Acting          D. L. Hughley\n",
              "12  1999        television actress  10/18/99    Acting       Rebecca Gayheart\n",
              "13  1999                  Comedian  10/19/99    Comedy          Steven Wright\n",
              "14  1999                   actress  10/20/99    Acting          Amy Brenneman\n",
              "15  1999                   actress  10/21/99    Acting        Melissa Gilbert\n",
              "16  1999                   actress  10/25/99    Acting         Cathy Moriarty\n",
              "17  1999                  comedian  10/26/99    Comedy         Louie Anderson\n",
              "18  1999                   actress  10/27/99    Acting  Sarah Michelle Gellar\n",
              "19  1999         Singer-songwriter  10/28/99  Musician              Melanie C"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "vtFAwNCQj0MH",
        "outputId": "e1937f83-61b3-45df-c357-97b3e8a6c093"
      },
      "source": [
        "superbowl_ads.head(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>brand</th>\n",
              "      <th>superbowl_ads_dot_com_url</th>\n",
              "      <th>youtube_url</th>\n",
              "      <th>funny</th>\n",
              "      <th>show_product_quickly</th>\n",
              "      <th>patriotic</th>\n",
              "      <th>celebrity</th>\n",
              "      <th>danger</th>\n",
              "      <th>animals</th>\n",
              "      <th>use_sex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018</td>\n",
              "      <td>Toyota</td>\n",
              "      <td>https://superbowl-ads.com/good-odds-toyota/</td>\n",
              "      <td>https://www.youtube.com/watch?v=zeBZvwYQ-hA</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020</td>\n",
              "      <td>Bud Light</td>\n",
              "      <td>https://superbowl-ads.com/2020-bud-light-seltz...</td>\n",
              "      <td>https://www.youtube.com/watch?v=nbbp0VW7z8w</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2006</td>\n",
              "      <td>Bud Light</td>\n",
              "      <td>https://superbowl-ads.com/2006-bud-light-bear-...</td>\n",
              "      <td>https://www.youtube.com/watch?v=yk0MQD5YgV8</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018</td>\n",
              "      <td>Hynudai</td>\n",
              "      <td>https://superbowl-ads.com/hope-detector-nfl-su...</td>\n",
              "      <td>https://www.youtube.com/watch?v=lNPccrGk77A</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003</td>\n",
              "      <td>Bud Light</td>\n",
              "      <td>https://superbowl-ads.com/2003-bud-light-hermi...</td>\n",
              "      <td>https://www.youtube.com/watch?v=ovQYgnXHooY</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2020</td>\n",
              "      <td>Toyota</td>\n",
              "      <td>https://superbowl-ads.com/2020-toyota-go-place...</td>\n",
              "      <td>https://www.youtube.com/watch?v=f34Ji70u3nk</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2020</td>\n",
              "      <td>Coca-Cola</td>\n",
              "      <td>https://superbowl-ads.com/2020-coca-cola-energ...</td>\n",
              "      <td>https://www.youtube.com/watch?v=-gAZRN3SCBw</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2020</td>\n",
              "      <td>Kia</td>\n",
              "      <td>https://superbowl-ads.com/2020-kia-tough-never...</td>\n",
              "      <td>https://www.youtube.com/watch?v=lMs79UXam9A</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2020</td>\n",
              "      <td>Hynudai</td>\n",
              "      <td>https://superbowl-ads.com/2020-hyundai-smaht-p...</td>\n",
              "      <td>https://www.youtube.com/watch?v=WBvkmWDjsYc</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2020</td>\n",
              "      <td>Budweiser</td>\n",
              "      <td>https://superbowl-ads.com/2020-budweiser-typic...</td>\n",
              "      <td>https://www.youtube.com/watch?v=J0xugdotpp8</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   year      brand  ... animals use_sex\n",
              "0  2018     Toyota  ...   False   False\n",
              "1  2020  Bud Light  ...   False   False\n",
              "2  2006  Bud Light  ...    True   False\n",
              "3  2018    Hynudai  ...   False   False\n",
              "4  2003  Bud Light  ...    True    True\n",
              "5  2020     Toyota  ...    True   False\n",
              "6  2020  Coca-Cola  ...    True   False\n",
              "7  2020        Kia  ...   False   False\n",
              "8  2020    Hynudai  ...    True   False\n",
              "9  2020  Budweiser  ...   False   False\n",
              "\n",
              "[10 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUrkidFF_K1z"
      },
      "source": [
        "# Your mission, should you choose to accept it...\n",
        "\n",
        "![mission impossible](https://66.media.tumblr.com/55af7b8e38e169303773d92d4c0e74a0/195050b77510a10c-f0/s640x960/679b8fd686c59e7fdd8d66c5e63965edeff54c9f.gif)\n",
        "\n",
        "Individually and/or in breakout groups, you'll use the functions and hints below to carry out a series of data science \"missions\" on the above datasets.  You'll have just a few minutes to tackle each mission.\n",
        "\n",
        "## Mission *goals*\n",
        "\n",
        "The overarching goals are to:\n",
        "- Increase familiarity with some advanced features of Pandas and Scikit-learn\n",
        "- Learn how to quickly read an API\n",
        "- Learn how to Google examples code snippets and adapt them to your needs\n",
        "- Communicate your thinking with others\n",
        "\n",
        "## Mission *parameters*\n",
        "\n",
        "I suggest that you attack each problem using the following heuristic:\n",
        "\n",
        "1. Make sure you understand the problem:\n",
        "  - What does the *input* or *data* look like?\n",
        "  - What does the *output* or *solution* look like?\n",
        "2. Understand which function(s) you're going to use:\n",
        "  - What libraries will you need?\n",
        "  - What's the syntax for using the function(s)?  (Hint: look up the library's API and then search for those functions!)\n",
        "  - What format does your dataset need to be in?  How can you wrangle it into that format?\n",
        "3. Create (or find!) a simple example of how to use the function(s)\n",
        "  - Good places to start your search: Google, Stack overflow, online tutorials or demos, GitHub repositories for relevant projects\n",
        "  - Don't worry (initially) about starting with the dataset you'll be applying your analysis to-- it may be easier to create a fake dataset (or download another existing dataset) that is already in the proper format.  You can use this as your \"dev\" dataset.\n",
        "4. Wrangle your dataset into the format you'll need it in.  This may involve:\n",
        "  - Removing rows/columns  \n",
        "  - Modifying existing values\n",
        "  - Filling in missing values\n",
        "  - Adding rows or columns, potentially based on values in other rows/columns\n",
        "  - Changing data types (e.g., converting strings to integers, etc.)\n",
        "    - It's worth spot-checking your dataset to make sure you *know* the datatype for each entry\n",
        "  - Make sure that values are consistent-- e.g., standardize capitalization, formats of dates, spelling, etc.  The `unique` function (in numpy or pandas) can be really useful here.\n",
        "5. Apply your function to your dataset:\n",
        "  - Start by copying the syntax you used in your \"dev\" dataset\n",
        "  - Then adjust or adapt any arguments or syntax to reflect your \"target\" (\"real\") dataset of interest\n",
        "6. Examine the result:\n",
        "  - Is the result the expected size?\n",
        "  - Do the rows and columns look right?\n",
        "  - Spot check a few entries (use `head` and `tail`, and then also select out a few rows/columns at random to check them).  More data implies that you should do more spot checks.  And if you have a small enough dataset (or if the dataset is important enough) you may want to manually check over the entire result.\n",
        "\n",
        "## Mission *completion*\n",
        "\n",
        "When you've achieved your mission objectives you should clean up your code so that you can prepare it for sharing with the class.  If you're working in a group, pick one person from your group to share their screen, and (roughly) plan out who will talk.  Aim for a quick debriefing-- try to take no more than 5 minutes (and ideally closer to 2ish minutes) to explain (a) your problem and (b) your solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94HOJMJFj-bB"
      },
      "source": [
        "# Mission list: Pandas\n",
        "\n",
        "## Titanic Dataset\n",
        "1. Use `groupby` (and, if needed, `apply`) to create a DataFrame of the *average* (hint: `groupby.mean`) probability of survival, fare, age, proportion of males, and probabilty of traveling alone by:\n",
        "  - Deck (`deck`)\n",
        "  - Class (`pclass`)\n",
        "2. Use `aggregate` to create a dataframe of the *minimum*, *median*, and *maximum* values for fare and age\n",
        "3. Create a series of bar plots showing survival probability by fare, age, deck, and class.  Split each bar based on passenger gender.\n",
        "\n",
        "## Daily Show Dataset\n",
        "4. Use `resample`, `apply`, and `rolling` to create a timeline of the proportion of guests from each category of profession (e.g., actor, comedian, model, politician, etc.) within a rolling 1-week window.  You may also find the `to_datetime` function useful.\n",
        "5. Same as the fourth mission, but use `expanding` instead of `rolling` to create a timeline of the *total* proportion of guests from each category of profession up to each successive date in the show.\n",
        "6. Create a bar plot showing the proportion of guests from each group (`Acting`, `Comedy`, `Musician`, etc.) by year.  Each year's group of bars should sum to 1.\n",
        "7. Create a bar plot showing the number of guests from each group (`Acting`, `Comedy`, `Musician`, etc.) by year.  Each year's group of bars should sum to the total number of guests from that year.\n",
        "\n",
        "## Superbowl Dataset\n",
        "8. Use `groupby`, `apply`, and `mean` to create a DataFrame indexed by *brand* of the proportions of ads by each company with each attrubute (`funny`, `show_product_quickly`, `patriotic`, `celebrity`, `danger`, `animals`, and `use_sex`)\n",
        "9. Create a DataFrame indexed by each *attribute* (`funny`, `show_product_quickly`, `patriotic`, `celebrity`, `danger`, `animals`, and `use_sex`), whose columns are the unique brands (`Toyota`, `Bud Light`, `Hynudai`, etc.).  Display the proportion of all ads with the given attribute (row) associated with each brand (column).  Hint: you can use row selection functions (`query`, `loc`, `iloc`, `xs`, `take`) to select out the rows with each given attribute.  Then use `aggregate` to compute proportions.  You may also find the `transform` and/or `stack` functions useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4NAUY3kA7IT"
      },
      "source": [
        "# Mission list: Scikit-learn\n",
        "\n",
        "## Titanic Dataset\n",
        "1. Train a `LogisticRegression` classifier to predict probability of survival based on the following attributes: `pclass`, `sex`, `age`, `fare`, `class`, `deck`, and `alone`.  Note: you'll need to first convert `sex` and `class` into numerical variables.  To obtain an ubiased estimate of classifier accuracy, randomly divide the data into a training and test set (use 75% of the data to train the model and evaluate performance on the remaining 25%).  Repeat this procedure 100 times (with new random assignments of training and test data each time) to obtain a distribution of classification accuracies (on the test data). The `train_test_split` function may be useful.\n",
        "2. Repeat the previous mission, but using the `CategoricalNB`, `DecisionTreeClassifier`, and `SVC` classifiers.  Create a bar graph showing average classification accuracy (with 95% confidence interval error bars) for each type of classifier (also include `LogisticRegression`).\n",
        "3. Use `KMeans` clustering to cluster participants into *k* = 3 groups according to the following features: `pclass`, `sex`, `age`, `fare`, `class`, `deck`, and `alone`:\n",
        "  - Create a separate DataFrame for each group\n",
        "  - Plot the probability of survival for each group\n",
        "  - Explore how the group assignments change with other clustering algorithms (e.g., `DBSCAN`, `Birch`)\n",
        "4. Create a feature vector for each passenger using the following features: `pclass`, `sex`, `age`, `fare`, `class`, `deck`, and `alone`.  Use `PCA` to project each passenger's feature vector onto two dimensions.  Create a scatterplot (component 1 vs. component 2) where each passenger's marker is assigned as follows:\n",
        "  - *shape*: a circle indicates that the passenger survived; an x indicates that the passenger died\n",
        "  - *size*: scale the marker sizes to indicate the fare paid by the passenger\n",
        "  - *color*: yellow = first class; orange = second class; red = third class\n",
        "5. Explore how the above scatterplot changes using the following dimensionality reduction algorithms: `DictionaryLearning`, `ICA`, `NMF`.\n",
        "\n",
        "## Superbowl Dataset\n",
        "6. Train `CategoricalNB` and `SVC` classifiers to predict `brand` from the following attributes: `funny`, `show_product_quickly`, `patriotic`, `celebrity`, `danger`, `animals`, and `use_sex`.  Create a bar graph showing the average classification accuracy (on held-out test data) for each type of classifier, with 95% confidence interval error bars.\n",
        "7. Create a `dendrogram` showing which brands create similar ads (based on the attributes `funny`, `show_product_quickly`, `patriotic`, `celebrity`, `danger`, `animals`, and `use_sex`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOz9Tu84I6B3"
      },
      "source": [
        "# Going Rogue...\n",
        "![going rogue](https://i.gifer.com/origin/4b/4bd1ef2830b4059cc55245e786a01cc9.gif)\n",
        "\n",
        "As your final mission, discover or explore something interesting about any of the datasets we've explored, using any of the features of `pandas` and/or `scikit-learn`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGu2tyJjI1Ow"
      },
      "source": [
        "# Concluding thoughts\n",
        "\n",
        "In attempting these problems, I'm hoping you've taken away a few key things:\n",
        "\n",
        "1. These libraries are feature-rich and useful for a wide range of applications.  I encourage you to think about how you might use these libraries in your research.\n",
        "2. Each library has its own \"feel\" or \"style\" to it.  The best way to build up intuitions about different libraries is to actually use them, ideally on real datasets, by turning questions about the data into analyses that you can implement using the given library.\n",
        "3. You'll get used to working with a subset of the functions from each library that you use most.  But for most other functions (and often even for the ones you know), you'll likely find yourself looking up syntax or examples very frequently.  Learning how to find what you want is way more useful and important than attempting to memorize syntax.\n",
        "\n",
        "We could easily take an entire term digging into any of these libraries.  This course can introduce you to some basic functionality and intuitions, but the \"next step\" in your learning journey is to learn how to \"train yourself\" to use these libraries (and others, including libraries that haven't been written yet!) by relying on your intuitions, prior knowledge about coding and other libraries, and any documentation or examples you can track down."
      ]
    }
  ]
}